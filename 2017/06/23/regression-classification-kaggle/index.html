<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.png?v=5.1.0" />






<meta name="description" content="详细探讨解决Kaggle上某回归/分类比赛的全过程。包括用线性回归之梯度下降法/正规方程法（MATLAB）、神经网络之多层感知器（TensorFlow）、最近邻（scikit-learn）进行回归预测和用逻辑回归之梯度上升法（Python）、梯度提升决策树（XGBoost）进行分类预测。读者将理解线性回归和逻辑回归的原理/实现、其他框架的使用/调参，以及如何利用Python的多进程对逻辑回归的运算">
<meta property="og:type" content="article">
<meta property="og:title" content="[数据挖掘] 回归和分类Kaggle实战">
<meta property="og:url" content="http://yoursite.com/2017/06/23/regression-classification-kaggle/index.html">
<meta property="og:site_name" content="HYPJUDY">
<meta property="og:description" content="详细探讨解决Kaggle上某回归/分类比赛的全过程。包括用线性回归之梯度下降法/正规方程法（MATLAB）、神经网络之多层感知器（TensorFlow）、最近邻（scikit-learn）进行回归预测和用逻辑回归之梯度上升法（Python）、梯度提升决策树（XGBoost）进行分类预测。读者将理解线性回归和逻辑回归的原理/实现、其他框架的使用/调参，以及如何利用Python的多进程对逻辑回归的运算">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/gd1.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/gd-slope.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/gd2.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/gdcost.jpg">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/normal-eq1.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/normal-eq2.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/mlp_network.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/scikit-learn-algorithm.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/regression-score.gif">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/regression-time.gif">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/sigmoid.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/L_theta.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/derivative-gradient-ascent.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/stochastic-gradient-descent.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/logistic-regression-gradient-descent.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/feature-scaling.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/LMS-rule.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/stochastic-gradient-descent.png">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/multiprocess-parallel-time.gif">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/logistic-regression-gradient-ascent-acc.gif">
<meta property="og:image" content="http://yoursite.com/images/reg-classification-kaggle/logistic-regression-gradient-ascent-time.gif">
<meta property="og:updated_time" content="2017-06-26T07:44:00.459Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[数据挖掘] 回归和分类Kaggle实战">
<meta name="twitter:description" content="详细探讨解决Kaggle上某回归/分类比赛的全过程。包括用线性回归之梯度下降法/正规方程法（MATLAB）、神经网络之多层感知器（TensorFlow）、最近邻（scikit-learn）进行回归预测和用逻辑回归之梯度上升法（Python）、梯度提升决策树（XGBoost）进行分类预测。读者将理解线性回归和逻辑回归的原理/实现、其他框架的使用/调参，以及如何利用Python的多进程对逻辑回归的运算">
<meta name="twitter:image" content="http://yoursite.com/images/reg-classification-kaggle/gd1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":20},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/06/23/regression-classification-kaggle/"/>





  <title> [数据挖掘] 回归和分类Kaggle实战 | HYPJUDY </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-96132499-1', 'auto');
  ga('send', 'pageview');
</script>











  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">HYPJUDY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">What I cannot create, I do not understand.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/23/regression-classification-kaggle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HYPJUDY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HYPJUDY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                [数据挖掘] 回归和分类Kaggle实战
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-23T12:54:07+08:00">
                2017-06-23
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/06/23/regression-classification-kaggle/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/23/regression-classification-kaggle/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>详细探讨解决<a href="https://www.kaggle.com/" target="_blank" rel="external">Kaggle</a>上某回归/分类比赛的全过程。包括用线性回归之梯度下降法/正规方程法（MATLAB）、神经网络之多层感知器（TensorFlow）、最近邻（scikit-learn）进行<strong>回归</strong>预测和用逻辑回归之梯度上升法（Python）、梯度提升决策树（XGBoost）进行<strong>分类</strong>预测。读者将理解线性回归和逻辑回归的<strong>原理/实现</strong>、其他框架的<strong>使用/调参</strong>，以及如何利用Python的多进程对逻辑回归的运算进行<strong>并行化</strong>提高效率。</p>
<a id="more"></a>
<p>★<a href="https://github.com/HYPJUDY/regression-classification-kaggle" target="_blank" rel="external"><strong>Source codes</strong></a></p>
<h1 id="目录-Table-of-Contents"><a href="#目录-Table-of-Contents" class="headerlink" title="目录(Table of Contents)"></a>目录(Table of Contents)</h1><p><em>请点击页面右下角的小图标查看详细目录</em></p>
<ol>
<li><a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#目录-Table-of-Contents" target="_blank" rel="external">目录(Table of Contents)</a></li>
<li><a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#回归-Regression" target="_blank" rel="external">回归(Regression)</a><br>2.1. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#数据-Data" target="_blank" rel="external">数据(Data)</a><br>2.2. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#度量标准-Evaluation" target="_blank" rel="external">度量标准(Evaluation)</a><br>2.3. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#线性回归-linear-regression" target="_blank" rel="external">线性回归(linear regression)</a><br>2.3.1. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#线性模型-linear-model" target="_blank" rel="external">线性模型(linear model)</a><br>2.3.2. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#线性回归-linear-regression-1" target="_blank" rel="external">线性回归(linear regression)</a><br>2.3.3. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#梯度下降法-Gradient-Descent" target="_blank" rel="external">梯度下降法(Gradient Descent)</a><br>2.3.4. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#正规方程-Normal-Equation" target="_blank" rel="external">正规方程(Normal Equation)</a><br>2.4. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#多层感知器-Multi-layer-Perceptron-MLP" target="_blank" rel="external">多层感知器(Multi-layer Perceptron, MLP)</a><br>2.5. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#最近邻之scikit-learn" target="_blank" rel="external">最近邻之scikit-learn</a><br>2.6. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#各回归方法对比" target="_blank" rel="external">各回归方法对比</a></li>
<li><a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#分类-Classification" target="_blank" rel="external">分类(Classification)</a><br>3.1. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#数据-Data-1" target="_blank" rel="external">数据(Data)</a><br>3.2. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#度量标准-Evaluation-1" target="_blank" rel="external">度量标准(Evaluation)</a><br>3.3. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#逻辑回归-Logistic-Regression" target="_blank" rel="external">逻辑回归(Logistic Regression)</a><br>3.3.1. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#Hypothesis-Representation" target="_blank" rel="external">Hypothesis Representation</a><br>3.3.2. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#梯度上升法" target="_blank" rel="external">梯度上升法</a><br>3.3.3. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#梯度下降法" target="_blank" rel="external">梯度下降法</a><br>3.3.4. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#梯度法分类及实现" target="_blank" rel="external">梯度法分类及实现</a><br>3.3.5. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#并行化" target="_blank" rel="external">并行化</a><br>3.3.6. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#结果及比较" target="_blank" rel="external">结果及比较</a><br>3.4. <a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#GBDT之XGBoost" target="_blank" rel="external">GBDT之XGBoost</a></li>
<li><a href="https://hypjudy.github.io/2017/06/23/regression-classification-kaggle/#Reference-amp-Futher-Reading" target="_blank" rel="external">Reference &amp; Futher Reading</a></li>
</ol>
<h1 id="回归-Regression"><a href="#回归-Regression" class="headerlink" title="回归(Regression)"></a>回归(Regression)</h1><blockquote>
<p>Predicting a <strong>continuous-valued</strong> attribute (continuous labels) associated with an object.</p>
</blockquote>
<p>回归预测的是<strong>连续</strong>的值。<br>Kaggle link: <a href="https://inclass.kaggle.com/c/linear-regression-sysu-2017/" target="_blank" rel="external">Linear Regression-SYSU-2017</a></p>
<h2 id="数据-Data"><a href="#数据-Data" class="headerlink" title="数据(Data)"></a>数据(Data)</h2><ol>
<li><code>save_train.csv</code><br> The format of each line is <code>id,value0,value1,...,value383,reference</code> where value0,value1,…,value383 are the features.</li>
<li><code>save_test.csv</code><br>This file contains the features whose references you need to predict. The format of each line is <code>id,value0,value1,...,value383</code>.</li>
<li><code>sample_submission.csv</code> (or <code>result.csv</code>)<br>The format of each line is <code>id,reference</code>.</li>
</ol>
<h2 id="度量标准-Evaluation"><a href="#度量标准-Evaluation" class="headerlink" title="度量标准(Evaluation)"></a>度量标准(Evaluation)</h2><p>RMSE (Root Mean Squared Error)<br>越小越好</p>
<h2 id="线性回归-linear-regression"><a href="#线性回归-linear-regression" class="headerlink" title="线性回归(linear regression)"></a>线性回归(linear regression)</h2><h3 id="线性模型-linear-model"><a href="#线性模型-linear-model" class="headerlink" title="线性模型(linear model)"></a>线性模型(linear model)</h3><p>给定由n个属性描述的示例$\mathbf{x}=(x_1;x_2;…;x_n)$，其中$x_i$是$\mathbf{x}$在第$i$个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即<br>$$h_{\mathbf{\theta}}(\mathbf{x})={\Theta}^T\mathbf{x}={\theta}_1x_1+{\theta}_2x_2+…+{\theta}_nx_n+b$$<br>$b$是常数项，通过引入$x_0=1$（截距intercept term），即令${\theta}_0x_0=b$，可以简化表达式。<br>$$h_{\mathbf{\theta}}(\mathbf{x})=\displaystyle \sum_{ i = 0 }^{ n } {\theta}_ix_i={\Theta}^T\mathbf{x}$$<br>$\Theta$和$b$学得之后，模型就得以确定。<br>线姓模型形式简单、易于建模，许多更为强大的非线性模型(nonlinear model)可在线性模型的基础上通过引入层次结构或高维映射得到。此外，由于$\Theta$直观地表达了各属性在预测中的重要性，线性模型有很好的可解释性(comprehensibility)。</p>
<h3 id="线性回归-linear-regression-1"><a href="#线性回归-linear-regression-1" class="headerlink" title="线性回归(linear regression)"></a>线性回归(linear regression)</h3><p>给定数据集$D=\{(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),…,(\mathbf{x}_m,y_m)\}$，其中$\mathbf{x}_i=(x_{i1};x_{i2};…;x_{in}),y_i \in \mathbb{ R }$.“线性回归”试图学得一个线性模型以尽可能准确地预测实值输出标记。即学得</p>
<p>$$h_{\mathbf{\theta}}(\mathbf{x})={\Theta}^T\mathbf{x},使得h_{\mathbf{\theta}}(x_i) \simeq y_i$$</p>
<p>为了刻画$h_{\mathbf{\theta}}(x_i)$和$y_i$之间的差距，定义代价函数(cost function)：</p>
<p>$$J(\theta)=\frac{1}{2}\displaystyle \sum_{ i = 1 }^{ m } (h_\theta(x^{(i)})-y^{(i)})^2$$</p>
<p>目标是找到使代价函数尽量小的$\theta$。具体的学习方法，可参考<a href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B01ARKEV1G" target="_blank" rel="external">《机器学习》第3章</a></p>
<p>接下来将讨论两种解线性回归的方法：梯度下降法和正规方程法。</p>
<h3 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法(Gradient Descent)"></a>梯度下降法(Gradient Descent)</h3><p>推荐阅读：<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">CS229 Lecture notes1 Part I</a></p>
<h4 id="算法概观"><a href="#算法概观" class="headerlink" title="算法概观"></a>算法概观</h4><p><img src="/images/reg-classification-kaggle/gd1.png" title="图一：梯度下降法, credit: Andrew Ng"><br>式子$h_{\theta}(x)={\theta}^Tx={\theta}_0x_0+{\theta}_1x_1+{\theta}_2x_2+…+{\theta}_nx_n$中的$x_1…x_n$是特征/变量，${\theta}_1…{\theta}_n$是待求参数。</p>
<p>梯度下降法是解决线性回归的一种方法。思路是选择一个初始的$\theta$值，并重复进行更新$\theta$以使得代价函数$J(\theta)$一直减小，直到达到极小值。具体算法是重复下面的步骤（同步更新$j=0,…,n$）：<br>$$\theta_j:=\theta_j-\alpha\frac{ \partial }{ \partial \theta_j }J(\theta)$$</p>
<p><a name="meaning-of-descent"></a></p>
<h4 id="梯度“下降”的含义"><a href="#梯度“下降”的含义" class="headerlink" title="梯度“下降”的含义"></a>梯度“下降”的含义</h4><p>更新规则（上式）中的<strong>减号</strong>用于<strong>最小化</strong>代价函数，这就是<strong>下降</strong>最直接的含义。<br>$\alpha$是学习率(learning rate)，梯度下降算法每次在$J$下降最快的方向走了一步，表现为找到的参数对应的代价“下降”。简化、直观的理解见下图：<br><img src="/images/reg-classification-kaggle/gd-slope.png" title="图二：迭代收敛图, credit: Andrew Ng"><br>图中的$\frac{ \partial }{ \partial \theta_j }J(\theta)$是偏导数(partial derivative)，Positive slope表示切线斜率(slope of the tangent)为正（反之为负），它们和梯度(gradient)的关系是</p>
<blockquote>
<p>If f(x1, …, xn) is a differentiable, real-valued function of several variables, its <strong>gradient</strong> is the vector whose components are the n <strong>partial derivatives</strong> of f. Like the derivative, the <strong>gradient</strong> represents the <strong>slope of the tangent</strong> of the graph of the function. (<a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="external">wiki</a>)</p>
</blockquote>
<p>值得注意的是，在梯度下降法的迭代过程中梯度也在下降：梯度是有方向的向量，由多个偏导数组成。梯度代表了切线斜率。从图二可以看出，无论梯度（切线斜率）初始为正还是负，在迭代过程中都会减小，减小到0时意味着找到了极小值，终止迭代。<br>但<strong>不能</strong>认为梯度上升法中的梯度在上升（读者可以一个向上凸的代价函数为例思考），<strong>“上升”</strong>可理解为<strong>加号</strong>用于<strong>最大化</strong>函数（在后文将讨论的逻辑回归法中指参数概率$L(\theta)$而不是<em>代价函数</em>，代价函数总是应该被最小化）。</p>
<h4 id="梯度“下降”证明"><a href="#梯度“下降”证明" class="headerlink" title="梯度“下降”证明"></a>梯度“下降”证明</h4><p><strong>梯度下降方法在选取的$\alpha$足够小、迭代运算次数足够多的情况下可以确保每次迭代$J$都减小并达到极小值。</strong>证明如下：<br>设代价函数上某点$\theta_1$，在$\alpha$<strong>足够小的正数</strong>时，迭代一步更新为$\theta_1^*=\theta_1-\alpha\frac{ \partial }{ \partial \theta_1}J(\theta_1)$.可认为$\theta_1$与$\theta_1^*$之间的曲线和曲线上点$\theta_1$处的切线重合，切线斜率$k$等于偏导数：<br>$$k=\frac{ \partial }{ \partial \theta_1 }J(\theta_1=\theta_1^*)$$ 又代价函数在$\theta_1$的值用点斜式可以表示为<br>$$J(\theta_1)=k\theta_1+b$$ 在$\theta_1^*$的值表示为<br>$$J(\theta_1^*)=k\theta_1^*+b=k(\theta_1-\alpha\frac{ \partial }{ \partial \theta_1}J(\theta_1))+b$$ 结合上面三个式子计算得到<br>$$J(\theta_1^*)-J(\theta_1)=\alpha(\frac{ \partial }{ \partial \theta_1}J(\theta_1))^2\geq0$$ 大部分情况下两者之差是正数，说明J在减小。若两者之差为0，即偏导数为0，说明已收敛到极小值处，可以停止计算了。</p>
<h4 id="计算偏导数"><a href="#计算偏导数" class="headerlink" title="计算偏导数"></a>计算偏导数</h4><p>图一中的偏导数进一步计算（此处省略计算过程）得到$\frac{ \partial }{ \partial y }J(\theta)=\frac{1}{m}\displaystyle \sum_{ i = 1 }^{ m } (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$，即下图<br><img src="/images/reg-classification-kaggle/gd2.png" title="图三：梯度下降法具体实现, credit: Andrew Ng"></p>
<h4 id="用MATLAB实现"><a href="#用MATLAB实现" class="headerlink" title="用MATLAB实现"></a>用MATLAB实现</h4><h5 id="读写数据及测试"><a href="#读写数据及测试" class="headerlink" title="读写数据及测试"></a>读写数据及测试</h5><p>注：这些是公共模块，后面不再赘述</p>
<p>读数据并给$\mathbf{x}$加上<a href="http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-to-interpret-the-constant-y-intercept" target="_blank" rel="external">常数项(intercept term)</a>：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">%% Load Data</span></div><div class="line">fprintf(<span class="string">'Loading data ...\n'</span>);</div><div class="line">train_data = csvread(<span class="string">'../data/save_train.csv'</span>, <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">X_train = train_data(:, <span class="number">1</span>:<span class="number">384</span>);</div><div class="line">y_train = train_data(:, <span class="number">385</span>);</div><div class="line">m = <span class="built_in">length</span>(y_train);</div><div class="line"></div><div class="line"><span class="comment">% Add intercept term to X</span></div><div class="line">X_train = [ones(m, <span class="number">1</span>) X_train];</div></pre></td></tr></table></figure>
<p>使用计算获得的$\theta$参数进行预测并写文件：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">%% Test and write to file</span></div><div class="line">fprintf(<span class="string">'Testing ...\n'</span>);</div><div class="line">X_test = csvread(<span class="string">'../data/save_test.csv'</span>, <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">X_test = [ones(m, <span class="number">1</span>) X_test];</div><div class="line">predict_data = X_test * theta;</div><div class="line">fprintf(<span class="string">'Writing to file ...\n'</span>);</div><div class="line">headers = &#123;<span class="string">'id'</span>,<span class="string">'reference'</span>&#125;;</div><div class="line">data = table((<span class="number">0</span>:<span class="number">24999</span>)', predict_data, <span class="string">'VariableNames'</span>, headers);</div><div class="line">writetable(data, <span class="string">'result.csv'</span>)</div></pre></td></tr></table></figure>
<h5 id="梯度下降实现"><a href="#梯度下降实现" class="headerlink" title="梯度下降实现"></a>梯度下降实现</h5><p>在实践中，数据应当进行预处理，将特征缩放(feature scaling)到相似的尺度（如$-1 \leq 1$），本例中的数据符合要求，因此跳过了这个步骤。<br>初始化。<code>alpha</code>及<code>num_iters</code>的值需根据不同的数据调整。本例中<code>alpha=0.1-0.001</code>可以较理想的速度收敛，<code>num_iters=50000</code>后收敛到局部极小值，难以再减小。下面减少迭代次数为500用于快速获得结果进行观察。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Init</span></div><div class="line">alpha = <span class="number">0.1</span> - <span class="number">0.001</span>;</div><div class="line">num_iters = <span class="number">500</span>; <span class="comment">%50000;</span></div><div class="line">theta = <span class="built_in">ones</span>(<span class="number">385</span>, <span class="number">1</span>);</div><div class="line"><span class="comment">% Run</span></div><div class="line">[theta, J_history] = ...</div><div class="line">    gradientDescentMulti(X_train, y_train, theta, alpha, num_iters);</div></pre></td></tr></table></figure>
<p>调用的<code>gradientDescentMulti</code>函数用于迭代学习$\theta$，实现如下：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = ...</span></div><div class="line">    gradientDescentMulti(X, y, theta, alpha, num_iters)</div><div class="line"><span class="comment">%GRADIENTDESCENTMULTI Performs gradient descent to learn theta</span></div><div class="line"><span class="comment">% theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates</span></div><div class="line"><span class="comment">% theta by taking num_iters gradient steps with learning rate alpha</span></div><div class="line"></div><div class="line"><span class="comment">% Initialize some useful values</span></div><div class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</div><div class="line">    newDecrement = alpha / m * (X * theta - y)' * X; </div><div class="line">    theta = theta - newDecrement';</div><div class="line">    </div><div class="line">    <span class="comment">% Save the cost J in every iteration   </span></div><div class="line">    <span class="comment">% Compute cost for linear regression with multiple variables</span></div><div class="line">    <span class="comment">% J_history(iter) = 1 / (2 * m) * sum((X * theta - y).^2);</span></div><div class="line">    J_history(iter) = <span class="built_in">sqrt</span>(sum((X * theta - y).^<span class="number">2</span>) / m); <span class="comment">% RMSE</span></div><div class="line">    fprintf(<span class="string">'RMSE: %f \n'</span>, J_history(iter));</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>需要注意的是，由于Kaggle的评价指标是标准误差(RMSE:Root Mean Squared Error)，默认的代价函数<br>$$J(\theta_0,\theta_1,…,\theta_n)=\frac{1}{2m}\displaystyle \sum_{ i = 1 }^{ m } (h_\theta(x^{(i)})-y^{(i)})^2$$</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J_history(iter) = <span class="number">1</span> / (<span class="number">2</span> * m) * sum((X * theta - y).^<span class="number">2</span>);</div></pre></td></tr></table></figure>
<p>被改为了<br>$$J(\theta_0,\theta_1,…,\theta_n)=\sqrt{\frac{\displaystyle \sum_{ i = 1 }^{ m } (h_\theta(x^{(i)})-y^{(i)})^2}{m}}$$</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J_history(iter) = <span class="built_in">sqrt</span>(sum((X * theta - y).^<span class="number">2</span>) / m);</div></pre></td></tr></table></figure>
<p>对结果没有影响。</p>
<h5 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h5><p>在训练集上进行测试是粗糙、不准确的，存在过拟合情况时尤其不可取。一种常用的办法是将训练集的一（小）部分作为验证集，剩下的部分用来训练。验证集和测试集最大的不同是验证集是有标签且可用于调参的，测试集即使有标签也不可用于调参。验证集不能直接被用于训练，但可以根据在验证集上测试的结果调整参数（或模型等）。<br>交叉验证是多次进行上面的过程，每次选取的验证集不同。例如将训练集分为三份ABC，第一轮A为验证集，BC为新的训练集；第二轮B为验证集，AC为新的训练集；第三轮C为验证集，AB为新的训练集。最后结果为三轮的平均。这样可充分利用数据、减少误差（避免数据不均匀），但同时增加了用时。<br>下面是进行交叉验证的梯度下降法在MATLAB的实现：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">alpha = <span class="number">0.1</span> - <span class="number">0.001</span>;</div><div class="line">num_iters = <span class="number">500</span>;</div><div class="line">tic</div><div class="line">k = <span class="number">3</span>;</div><div class="line">cv = cvpartition(y_train, <span class="string">'Kfold'</span>, k);</div><div class="line">mse = <span class="built_in">zeros</span>(k, <span class="number">1</span>);</div><div class="line"><span class="keyword">for</span> k = <span class="number">1</span> : k</div><div class="line">    trainIdx = cv.training(k);</div><div class="line">    testIdx = cv.test(k);</div><div class="line">    theta = <span class="built_in">zeros</span>(<span class="number">385</span>, <span class="number">1</span>);</div><div class="line">    [theta, J_history] = gradientDescentMulti(X_train(trainIdx,:),...</div><div class="line">        y_train(trainIdx), theta, alpha, num_iters);</div><div class="line">     </div><div class="line">    y_hat = X_train(testIdx, :) * theta;</div><div class="line">    </div><div class="line">    mse(k) = mean((y_train(testIdx) - y_hat).^<span class="number">2</span>);</div><div class="line">    fprintf(<span class="string">'Fold %d mse: %f \n'</span>, k, mse(k));</div><div class="line"><span class="keyword">end</span></div><div class="line">toc</div><div class="line">avrg_rmse = mean(<span class="built_in">sqrt</span>(mse));</div><div class="line">fprintf(<span class="string">'avrg_rmse: %f \n'</span>, avrg_rmse);</div></pre></td></tr></table></figure>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>将梯度下降迭代的中间结果制图有助于观察收敛情况，选取合适的参数。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Plot</span></div><div class="line">figure;</div><div class="line">plot(<span class="number">1</span>:<span class="built_in">numel</span>(J_history), J_history, <span class="string">'-b'</span>, <span class="string">'LineWidth'</span>, <span class="number">2</span>); <span class="comment">% blue</span></div><div class="line">xlabel(<span class="string">'Number of iterations'</span>);</div><div class="line">ylabel(<span class="string">'Cost J'</span>);</div></pre></td></tr></table></figure>
<p><img src="/images/reg-classification-kaggle/gdcost.jpg" title="图四：迭代过程图"></p>
<p>在迭代150次后，收敛速度开始显著下降，迭代500次所用的时间和<strong>在训练集上</strong>测试的输出为</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% ----------- Output -------------</span></div><div class="line"><span class="comment">% Elapsed time: 9.203286 seconds</span></div><div class="line"><span class="comment">% RMSE on train dataset: 8.562877</span></div></pre></td></tr></table></figure>
<p>交叉验证的输出为：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% ----------- Output -------------</span></div><div class="line"><span class="comment">% Elapsed time: 19.245382 seconds</span></div><div class="line"><span class="comment">% avrg_rmse: 10.150006</span></div></pre></td></tr></table></figure>
<p>交叉验证的结果往往差于在训练集上测试的结果，与真实测试结果（Kaggle提交<code>score=8.50819</code>）更为接近，可能好/差于真实测试结果。</p>
<h3 id="正规方程-Normal-Equation"><a href="#正规方程-Normal-Equation" class="headerlink" title="正规方程(Normal Equation)"></a>正规方程(Normal Equation)</h3><blockquote>
<p>Normal equation: Method to solve for $\theta$ analytically which minimize $J$ by<br>explicitly taking its derivatives with respect to the $\theta_j$’s, and setting them to zero.</p>
</blockquote>
<h4 id="算法概观-1"><a href="#算法概观-1" class="headerlink" title="算法概观"></a>算法概观</h4><p>和梯度下降不同，为了找到代价函数的最小值对应的$\theta$，正规方程法不需要进行多次迭代，而是直接计算极小值点。极小值的计算和计算代价函数上导数为0的点有关。<br><img src="/images/reg-classification-kaggle/normal-eq1.png" title="图五：正规方程Intuition, credit: Andrew Ng"></p>
<h4 id="计算参数"><a href="#计算参数" class="headerlink" title="计算参数"></a>计算参数</h4><p>$\theta$的计算较复杂，涉及到矩阵的求导、最小二乘法修正，有兴趣的读者请参考<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">CS229 Lecture notes1 Part I - 2</a><br><img src="/images/reg-classification-kaggle/normal-eq2.png" title="图六：正规方程公式, credit: Andrew Ng"><br>下面提供一种简化的推导：<br>沿用图六设定的符号，<br>$$X\theta=y \quad(1)\\<br>X^TX\theta=X^Ty \quad(2)\\<br>(X^TX+\lambda I)\theta=X^Ty \quad(3)\\<br>\theta=(X^TX+\lambda I)^{-1}X^Ty \quad(4)$$<br>求$\theta$可以转化为求(1)式；(2)式的$X^TX$是$n \times n$矩阵（才可能有逆矩阵）；(3)式的$\lambda$是很小的常数，$I$是单位矩阵，两者相乘得到的矩阵加到$X^TX$上影响可忽略不计，$X^TX+\lambda I$一定有逆矩阵，保证了(4)式的正确性。有两种情况$X^TX$不可逆：</p>
<ol>
<li>有重复的特征（线性相关）。</li>
<li>特征数过多。</li>
</ol>
<p>由于这两种情况很少发生，常用下式表示$\theta$：$$\theta=(X^TX)^{-1}X^Ty \quad(4)$$</p>
<h4 id="MATLAB实现"><a href="#MATLAB实现" class="headerlink" title="MATLAB实现"></a>MATLAB实现</h4><p>虽然正规方程的推导繁琐，但实现仅需一行代码：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">theta = pinv(X_train' * X_train) * X_train' * y_train;</div></pre></td></tr></table></figure>
<ul>
<li>MATLAB中的<code>pinv</code>求逆函数处理了<code>X_train&#39; * X_train</code>不可逆的情况，因此不需另外加上$\lambda I$项。</li>
<li>不需要进行特征缩放预处理</li>
</ul>
<p>交叉验证也很简洁：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">k = <span class="number">3</span>;</div><div class="line">cv = cvpartition(y_train, <span class="string">'Kfold'</span>, k);</div><div class="line">mse = <span class="built_in">zeros</span>(k, <span class="number">1</span>);</div><div class="line"><span class="keyword">for</span> k = <span class="number">1</span> : k</div><div class="line">    trainIdx = cv.training(k);</div><div class="line">    testIdx = cv.test(k);</div><div class="line">    theta = pinv(X_train(trainIdx, :)' * X_train(trainIdx, :)) ...</div><div class="line">        * X_train(trainIdx, :)' * y_train(trainIdx);</div><div class="line">    y_hat = X_train(testIdx, :) * theta;</div><div class="line">    </div><div class="line">    mse(k) = mean((y_train(testIdx) - y_hat).^<span class="number">2</span>);</div><div class="line">    fprintf(<span class="string">'Fold %d mse: %f \n'</span>, k, mse(k));</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% ----------- Output -------------</span></div><div class="line"><span class="comment">% Elapsed time: 0.368472 seconds</span></div><div class="line"><span class="comment">% Normal equation RMSE on train dataset: 8.170871</span></div></pre></td></tr></table></figure>
<p>交叉验证的输出为：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% ----------- Output -------------</span></div><div class="line"><span class="comment">% Elapsed time: 1.136081 seconds</span></div><div class="line"><span class="comment">% avrg_rmse: 8.325153</span></div></pre></td></tr></table></figure>
<p>Kaggle提交的真实测试结果<code>score=8.44464</code>，可见该例中正规方程法无论是用时还是误差都优于梯度下降法</p>
<h2 id="多层感知器-Multi-layer-Perceptron-MLP"><a href="#多层感知器-Multi-layer-Perceptron-MLP" class="headerlink" title="多层感知器(Multi-layer Perceptron, MLP)"></a>多层感知器(Multi-layer Perceptron, MLP)</h2><p>上文中使用了线性回归的方法进行拟合，效果不好，原因可能是数据量大、特征不多，线性模型的表达能力较弱。因此尝试非线性的神经网络。多层感知器是其中最简单的一种。</p>
<h3 id="模型概观"><a href="#模型概观" class="headerlink" title="模型概观"></a>模型概观</h3><p>MLP也称为前馈人工神经网络(feedforward neural network)，网络的输入层(input layer)和输出层(output layer)之间有一或多层隐藏层(hidden layer)。前馈意为数据从输入层流向输出层(forward)。这种类型的网络用后向传播(backpropagation)学习算法进行训练的。<br><img src="/images/reg-classification-kaggle/mlp_network.png" title="图七：单隐藏层感知机, credit: Scikit learn" width="50%" style="margin: auto;"><br>最左边的层即输入层，由一组神经元(neuron)${x_i|x_1,x_2,…,x_m}$代表输入特征。隐藏层中的每个神经元的值通过将前一层中的值通过加权线性求和$w_1x_1+w_2x_2+…+w_mx_m$，再用一个非线性激活函数(non-linear activation function)$g(\cdot):R\rightarrow R$获得。输出层接收上一层的值并转换成输出值。输入输出层均可以有多个神经元。<br>可以看出多层感知器和线性回归模型最大的区别在于其通过两种方式实现的“非线性”：</p>
<ol>
<li>非线性激活函数</li>
<li>多层组合</li>
</ol>
<h3 id="用TensorFlow实现"><a href="#用TensorFlow实现" class="headerlink" title="用TensorFlow实现"></a>用TensorFlow实现</h3><h4 id="主框架"><a href="#主框架" class="headerlink" title="主框架"></a>主框架</h4><p>下面是用TensorFlow实现(Python语言）多层感知器的主要过程，包含了读写数据、训练、测试，可分为9个步骤(step)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># Step 1: Read data</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Reading training data...'</span></div><div class="line">    train_data_batch, train_label_batch = train_batch_generator([TRAIN_PATH])</div><div class="line">    train_features, train_labels = \</div><div class="line">            train_generate_batches(train_data_batch, train_label_batch)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">'Reading testing data...'</span></div><div class="line">    test_data_batch = test_batch_generator([TEST_PATH])</div><div class="line">    test_features = test_generate_batches(test_data_batch)</div><div class="line"></div><div class="line">    <span class="comment"># Step 2: create placeholders for features and labels</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'data'</span>):</div><div class="line">        X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">384</span>], name=<span class="string">"X_placeholder"</span>)</div><div class="line">        Y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"Y_placeholder"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Step 3: create weight and bias</span></div><div class="line">    w1 = tf.Variable(xavier_init(<span class="number">384</span>, NODE_NUM), name=<span class="string">'weights1'</span>)</div><div class="line">    b1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[NODE_NUM], dtype=tf.float32), name=<span class="string">'bias1'</span>)</div><div class="line">    w2 = tf.Variable(xavier_init(NODE_NUM, NODE_NUM), name=<span class="string">'weights2'</span>)</div><div class="line">    b2 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[NODE_NUM], dtype=tf.float32), name=<span class="string">'bias2'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Step 4: build model to predict Y</span></div><div class="line">    hidden = tf.sigmoid(tf.add(tf.matmul(X, w1), b1))</div><div class="line">    Y_predicted = tf.add(tf.matmul(hidden, w2), b2)</div><div class="line"></div><div class="line">    <span class="comment"># Step 5: use the square error as the loss function</span></div><div class="line">    loss = tf.reduce_mean(tf.square(Y - Y_predicted), name=<span class="string">'loss'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Step 6: using gradient descent to minimize loss</span></div><div class="line">    optimizer = \</div><div class="line">    tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)</div><div class="line"></div><div class="line">    <span class="comment"># Launch the graph and train</span></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        <span class="comment"># Step 7: initialize the necessary variables, in this case, w and b</span></div><div class="line">        sess.run(tf.global_variables_initializer()) </div><div class="line">    </div><div class="line">        writer = tf.summary.FileWriter(<span class="string">'./graph'</span>, sess.graph)</div><div class="line">    </div><div class="line">        <span class="comment"># Step 8: train the model</span></div><div class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> xrange(MAX_STEP):</div><div class="line">            <span class="comment"># Session runs train_op and fetch values of loss</span></div><div class="line">            _, l = sess.run([optimizer, loss], \</div><div class="line">                            feed_dict=&#123;X:train_features, Y:train_labels&#125;) </div><div class="line">            <span class="keyword">print</span> <span class="string">'Step &#123;0&#125;: loss = &#123;1&#125;'</span>.format(step, np.sqrt(l)) <span class="comment"># loss &lt;- RMSE</span></div><div class="line">            </div><div class="line">            <span class="comment"># Step 9: Test the model every SAVE_ITERS steps</span></div><div class="line">            <span class="keyword">if</span> (step + <span class="number">1</span>) % SAVE_ITERS == <span class="number">0</span>:</div><div class="line">                result = sess.run(Y_predicted, feed_dict=&#123;X:test_features&#125;)</div><div class="line">                filename = RESULT_PATH + <span class="string">'_'</span> + str(step) + \</div><div class="line">                           <span class="string">'_'</span> + str(int(np.sqrt(l)*<span class="number">100</span>))+ <span class="string">'.csv'</span></div><div class="line">                <span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</div><div class="line">                    csv_w = csv.writer(fw)</div><div class="line">                    csv_w.writerow([<span class="string">'Id'</span>, <span class="string">'reference'</span>])</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(BATCH_SIZE):</div><div class="line">                        csv_w.writerow([i, result[i][<span class="number">0</span>]])</div><div class="line"></div><div class="line">        <span class="comment"># close the writer when you're done using it</span></div><div class="line">        writer.close()</div></pre></td></tr></table></figure>
<p>上面的代码对MLP的设置如下：</p>
<ul>
<li><code>step 3</code>、<code>step 4</code>定义了MLP的结构：有1个隐藏层，输入层有384（特征数）个神经元，输出层有1个神经元，隐藏层的神经元数为<code>NODE_NUM=50</code>，激活函数为<code>sigmoid</code></li>
<li><code>step 6</code>的<code>GradientDescentOptimizer</code>表示用梯度下降法来减少损失，<code>LEARNING_RATE = 0.001</code></li>
<li><code>step 8</code>的<code>MAX_STEP=4000</code>：迭代4000步<br>测试结果（Kaggle提交）<code>score=5.62558</code>，可见非线性模型的拟合能力远优于线性回归（<code>score=8.44464</code>）。</li>
</ul>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>下面在此基础上改进多层感知机以获得更好的结果（每一个改动都在上一个的基础上）。</p>
<ol>
<li>增加迭代次数<code>MAX_STEP=30000</code>：<code>score=1.77183</code>.</li>
<li><p>增加隐藏层数为四层如下：<code>score=1.58680</code>.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Step 3: create weight and bias</span></div><div class="line">w1 = tf.Variable(xavier_init(<span class="number">384</span>, NODE_NUM), name=<span class="string">'weights1'</span>)</div><div class="line">b1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[NODE_NUM], dtype=tf.float32), name=<span class="string">'bias1'</span>)</div><div class="line">w2 = tf.Variable(xavier_init(NODE_NUM, NODE_NUM), name=<span class="string">'weights2'</span>)</div><div class="line">b2 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[NODE_NUM], dtype=tf.float32), name=<span class="string">'bias2'</span>)</div><div class="line">w3 = tf.Variable(xavier_init(NODE_NUM, NODE_NUM), name=<span class="string">'weights3'</span>)</div><div class="line">b3 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[NODE_NUM], dtype=tf.float32), name=<span class="string">'bias3'</span>)</div><div class="line">w4 = tf.Variable(xavier_init(NODE_NUM, NODE_NUM), name=<span class="string">'weights4'</span>)</div><div class="line">b4 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[NODE_NUM], dtype=tf.float32), name=<span class="string">'bias4'</span>)</div><div class="line">w5 = tf.Variable(xavier_init(NODE_NUM, <span class="number">1</span>), name=<span class="string">'weights5'</span>)</div><div class="line">b5 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>], dtype=tf.float32), name=<span class="string">'bias5'</span>)</div><div class="line"><span class="comment"># Step 4: build model to predict Y</span></div><div class="line">hidden = tf.sigmoid(tf.add(tf.matmul(X, w1), b1))</div><div class="line">hidden2 = tf.sigmoid(tf.add(tf.matmul(hidden, w2), b2))</div><div class="line">hidden3 = tf.sigmoid(tf.add(tf.matmul(hidden2, w3), b3))</div><div class="line">hidden4 = tf.sigmoid(tf.add(tf.matmul(hidden3, w4), b4))</div><div class="line">Y_predicted = tf.add(tf.matmul(hidden4, w5), b5)</div></pre></td></tr></table></figure>
</li>
<li><p>进行三个改动：</p>
<ul>
<li><code>step 4</code>的激活函数改为<code>relu</code>：如<br><code>hidden = tf.nn.relu(tf.add(tf.matmul(X, w1), b1))</code></li>
<li><code>step 6</code>的<code>learning_rate</code>改为动态减小，<code>STARTER_LEARNING_RATE = 1e-2</code>：<br><code>learning_rate = tf.train.exponential_decay(STARTER_LEARNING_RATE, global_step, DECAY_STEPS, DECAY_RATE, staircase=True)</code></li>
<li><code>step 6</code>的<code>optimizer</code>改为<code>AdamOptimizer</code>(<a href="https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow" target="_blank" rel="external">区别</a>)：<br><code>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)</code><br>这样的设置下在<code>MAX_STEP=19000</code>时即得到了更优的<code>score=1.15590</code>，说明激活函数<code>relu</code>优于<code>sigmoid</code>，训练过程中改变学习率即可加速收敛又利于达到极小值。</li>
</ul>
</li>
<li>加大隐藏层神经元数<code>NODE_NUM=100</code>，训练<code>MAX_STEP=27000</code>次：<code>score=1.09160</code>.</li>
<li>设置隐藏层神经元数各层不一，分别为300、150、50、10，在<code>MAX_STEP=17000</code>时即得到了更优的：<code>score=0.91748</code>.</li>
</ol>
<p>可以发现，从<code>score=5.62558</code>到<code>score=0.91748</code>，一开始进步很快，后来举步维艰，已经很难再改进。神经网络迭代次数上万，在多个GPU共同训练的情况下也耗时较多。另一点是调参虽有一定的经验规律可用，但整体还是个黑箱，不容易找到较好的参数。听闻神经网络法在某种繁复的设置下（如采用类似VGG的结构）还是可以达到<code>score~=0.24</code>的，读者有兴趣可进行尝试。</p>
<h2 id="最近邻之scikit-learn"><a href="#最近邻之scikit-learn" class="headerlink" title="最近邻之scikit-learn"></a>最近邻之scikit-learn</h2><p>经试验，MLP虽然远优于线性回归，但是也难以获得<code>score&lt;0.5</code>，回头尝试其他传统回归方法。机器学习库scikit-learn已经造了大量轮子，下面介绍用scikit-learn中的<a href="https://scikit-learn.org/stable/modules/neighbors.html" target="_blank" rel="external">最近邻</a>算法及一个trick获得<code>score=0.24633</code>，其他<a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" target="_blank" rel="external">回归方法</a>亦可照葫芦画瓢。<br><img src="/images/reg-classification-kaggle/scikit-learn-algorithm.png" title="图八：scikit-learn算法选择, credit: scikit learn" style="margin: auto;"></p>
<h3 id="KNN概览"><a href="#KNN概览" class="headerlink" title="KNN概览"></a>KNN概览</h3><p>最近邻算法是一种非常直观朴素的算法，对于回归任务，预测一个样本时从特征空间中选择距离该样本最近（最相似）的k个标注样本，该样本的预测值为这些邻居的平均值。如果是分类任务，则预测为大多数邻居所属的类别。<br>如果采用暴力搜索来找邻居，大数据量下将非常耗时，因此常用一些快速索引结构如Ball Tree或KD Tree。在scikit-learn中只需改<code>algorithm</code>参数即可。<br>另外“近”可以由不同的标准定义，如欧氏距离、曼哈顿距离；对于不同相近程度的邻居也可以赋予不同的权重或者一视同仁。更多的参数，可参看<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" target="_blank" rel="external">sklearn.neighbors.KNeighborsRegressor文档</a></p>
<h3 id="用scikit-learn实现"><a href="#用scikit-learn实现" class="headerlink" title="用scikit-learn实现"></a>用scikit-learn实现</h3><p>可参考<a href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py" target="_blank" rel="external">Nearest Neighbors regression Example</a>(Python语言），下面为主程序代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># Step 1: Read data</span></div><div class="line">    train_data_batch, train_label_batch = train_batch_generator([TRAIN_PATH])</div><div class="line">    train_features, train_labels = \</div><div class="line">        train_generate_batches(train_data_batch, train_label_batch)</div><div class="line"></div><div class="line">    test_data_batch = test_batch_generator([TEST_PATH])</div><div class="line">    test_features = test_generate_batches(test_data_batch)</div><div class="line">    </div><div class="line">    <span class="comment"># Step 2: Define model</span></div><div class="line">    KNN = neighbors.KNeighborsRegressor(n_neighbors=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Step 3: Fit the model and predict</span></div><div class="line">    test_labels = knn.fit(train_features, train_labels).predict(test_features)</div><div class="line">    </div><div class="line">    <span class="comment"># Step 4: Write file</span></div><div class="line">    <span class="keyword">with</span> open(<span class="string">'res_nn.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</div><div class="line">        csv_w = csv.writer(fw)</div><div class="line">        csv_w.writerow([<span class="string">'Id'</span>, <span class="string">'reference'</span>])</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(BATCH_SIZE):</div><div class="line">            csv_w.writerow([i, test_labels[i][<span class="number">0</span>]])</div></pre></td></tr></table></figure>
<p><code>step 1</code>、<code>step 4</code>是普通的读写数据，和上文MLP所用代码一样。核心的代码只有<code>step 2</code>、<code>step 3</code>的两行，分别用于定义最近邻回归模型和进行训练、预测。<code>n_neighbors</code>默认为5，设置为1可以获得单个最近邻模型的最好结果<code>score=0.26200</code>，相比于MLP获得了突飞猛进的进步。如何找到<code>n_neighbors=1</code>这个“最好”的参数呢？可分割训练集测试。<br>scikit-learn依然提供了非常方便的方法。在定义了模型之后，可先使用<code>train_test_split</code>方法分割带标注的数据（原训练集）为训练集和测试集，再用<code>fit</code>方法训练，<code>score</code>方法测试获得分数（此处<code>score</code>默认的不是RMSE，但是能反映模型好坏）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">X_train, X_test, y_train, y_test = train_test_split(\</div><div class="line">    train_features, train_labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="keyword">print</span> knn.fit(X_train, y_train).score(X_test, y_test)</div></pre></td></tr></table></figure>
<p>在训练集数据量不是很大或者想获得更均匀准确的估测结果的情况下，可以使用交叉验证（缺点是耗时相对长）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scores = cross_val_score(knn, train_features, train_labels, \</div><div class="line">    cv=<span class="number">10</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</div><div class="line"><span class="keyword">print</span> (scores, np.sqrt(np.abs(scores.mean()))) <span class="comment"># RMSE</span></div></pre></td></tr></table></figure>
<h3 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h3><p>从提交结果看，nn(k=1)和2nn(k=2)的结果最好，相差0.01.下面采用的Trick姑且称为杂交优势：</p>
<table>
<thead>
<tr>
<th><strong>knn</strong></th>
<th>2nn</th>
<th>nn</th>
<th>$0.5*nn+0.5*2nn$</th>
<th>$0.6*nn+0.4*2nn$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>score</strong></td>
<td>0.27622</td>
<td>0.26200</td>
<td>0.24697</td>
<td>0.24635</td>
</tr>
</tbody>
</table>
<p>这种方法确实有偶然性，但也是有依据的：赋予不同距离的邻居不同的权重。设最近的邻居为$f$，第二近的邻居为$s$，由于前面采用的是默认的<code>weights=&#39;uniform&#39;</code>即全部邻居全部相同权重，可以看做$2nn=(f+s)*0.5$，因此下面这个例子可以拆分成<br>$$0.6*nn+0.4*2nn=0.6*(f)+0.4*(f+s)*0.5=0.8*f+0.2*s$$ 也就是以4:1的权重来对待$f$和$s$，相当于自定义了<code>weights</code>.</p>
<p>其实，这种方法可视为<strong>ensembling</strong>方法中最简单的一种，具体请参考<a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="external">KAGGLE ENSEMBLING GUIDE</a>。</p>
<h2 id="各回归方法对比"><a href="#各回归方法对比" class="headerlink" title="各回归方法对比"></a>各回归方法对比</h2><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/regression-score.gif" alt=""></div></div><div class="group-picture-row"></div></div></div>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/regression-time.gif" alt=""></div></div><div class="group-picture-row"></div></div></div>
<p><strong>结论：</strong>线性回归方法误差过高，淘汰；KNN又快又准，胜出；神经网络再接再厉，兴可推前浪。</p>
<h1 id="分类-Classification"><a href="#分类-Classification" class="headerlink" title="分类(Classification)"></a>分类(Classification)</h1><blockquote>
<p>Identifying to which <strong>category</strong> (discrete labels) an object belongs to.</p>
</blockquote>
<p>分类预测的是<strong>离散</strong>的值（类别）。<br>Kaggle link: <a href="https://inclass.kaggle.com/c/large-scale-classification-sysu-2017/" target="_blank" rel="external">Large-scale classification-SYSU-2017</a></p>
<p>该例中是二分类问题，$y\in \{0,1\}$</p>
<h2 id="数据-Data-1"><a href="#数据-Data-1" class="headerlink" title="数据(Data)"></a>数据(Data)</h2><p>1.<code>train_data.txt</code><br>  The format of each line is <code>label index1:value1 index2:value2 index4:value4 ...</code> where value1,value2,… are the features(the ignored value equals 0 e.g. the value3 here equals 0) and there are only 2 classes(label) in all,indexed from 0 to 1.<br>2.<code>test_data.txt</code><br>  This file contains the features without labels that you need to predict. The format of each line is <code>id index1:value1 index2:value2 index3:value3 ...</code><br>3.<code>sample_submission.txt</code> (or <code>result.txt</code>)<br>  The file you submit should have the same format as this file,the format of each line is <code>id,label</code></p>
<h2 id="度量标准-Evaluation-1"><a href="#度量标准-Evaluation-1" class="headerlink" title="度量标准(Evaluation)"></a>度量标准(Evaluation)</h2><p>AUC(Area under Receiver Operating Characteristic Curve)[binary-classification]<br>越大越好</p>
<h2 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归(Logistic Regression)"></a>逻辑回归(Logistic Regression)</h2><p>推荐阅读：<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">CS229 Lecture notes1 Part II</a></p>
<p>逻辑“回归”是用于<strong>分类</strong>任务而非回归任务的。</p>
<h3 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h3><p>联想一开头讨论的线性回归算法，求解分类问题可以先忽视$y$是离散值，而套用线性回归算法给定$x$预测$y$.此思路最大的问题在于预测的连续值大于1或小于0时都无意义，因为$y\in \{0,1\}$.然而，通过<strong>逻辑函数(logistic function)</strong>（或<strong>sigmoid函数</strong>），可以将范围限制在0和1之间：<br>$$h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}},\\<br>where \quad g(z)=\frac{1}{1+e^{-z}},\\<br>\theta^Tx=\theta_0+\sum_{ j = 1 }^{ n } \theta_jx_j$$<br><img src="/images/reg-classification-kaggle/sigmoid.png" title="图九：逻辑函数/sigmoid函数, credit: Andrew Ng" style="margin:auto;"><br>其他能将值平滑映射为从0增加到1的函数也可，sigmoid函数直观简单，它的导数也有着很好的形式：<br>$$<br>\begin{eqnarray}<br>g’(z) &amp;=&amp; \frac{d}{dz} \frac{1}{1+e^{-z}}\\<br>      &amp;=&amp; \frac{1}{(1+e^{-z})^2}(e^{-z})\\<br>      &amp;=&amp; \frac{1}{(1+e^{-z})} \cdot (1-\frac{1}{(1+e^{-z})})\\<br>      &amp;=&amp; g(z)(1-g(z))<br>\end{eqnarray}<br>$$</p>
<h3 id="梯度上升法"><a href="#梯度上升法" class="headerlink" title="梯度上升法"></a>梯度上升法</h3><h4 id="Likelihood-of-the-Parameters"><a href="#Likelihood-of-the-Parameters" class="headerlink" title="Likelihood of the Parameters"></a>Likelihood of the Parameters</h4><p>假设<br>$$\begin{eqnarray}<br>P(y=1|x;\theta)&amp;=&amp;h_{\theta}(x)\\<br>P(y=0|x;\theta)&amp;=&amp;1-h_{\theta}(x)<br>\end{eqnarray}$$</p>
<p>$P$是概率/可能性(likelihood)，我们的目标是<strong>最大化</strong>$P$。因为假设在$P$最大的边界情况下$P=1$，根据假设：</p>
<ul>
<li>当一个样本的真实标签$y=1$时，这个样本的预测值$h_{\theta}(x)=1$，预测正确；</li>
<li>当一个样本的真实标签$y=0$时，这个样本的预测值$h_{\theta}(x)=0$，预测正确。</li>
</ul>
<p>接下来皆使用一个更简洁的式子来代替上面两个式子的表示：<br>$$P(y|x;\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{1-y}$$</p>
<p>假设m个训练样本各自独立生成，从而可以统一表示所有参数的这种可能性(the likelihood of the parameters)：<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/L_theta.png" alt=""></div></div><div class="group-picture-row"></div></div></div></p>
<h4 id="Maximize-the-Likelihood-Gradient-Ascent"><a href="#Maximize-the-Likelihood-Gradient-Ascent" class="headerlink" title="Maximize the Likelihood (Gradient Ascent)"></a>Maximize the Likelihood (Gradient Ascent)</h4><p>首先对$L(\theta)$做对数运算，这样有利于求最大值<br>$$\begin{eqnarray}<br>l(\theta) &amp;=&amp; \log L(\theta)\\<br>&amp;=&amp; \sum_{i=1}^{m} y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))<br>\end{eqnarray}$$</p>
<p>和线性回归的求解思路（梯度上升法）相似，下面采用<strong>梯度上升法</strong>来最大化(likelihood)函数，更新规则如下：<br>$$\theta:=\theta+\alpha {\nabla}_{\theta} l(\theta)$$<br>(用<strong>正号</strong>而非符号是因为此时我们需要<strong>最大化</strong>而非最小化函数，更多相关解释，可回到上文的<a href="#meaning-of-descent">梯度“上升”的含义</a>)</p>
<p>以<em>一个样本</em>$(x,y)$为例展开求导（利用了前面推导的sigmoid函数的导数$g’(z)=g(z)(1-g(z))$和$(lnx)’=1/x$）：<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/derivative-gradient-ascent.png" alt=""></div></div><div class="group-picture-row"></div></div></div><br>求得<em>随机</em>梯度上升规则<br>$$\theta_j:=\theta_j+\alpha (y^{(i)}-h_{\theta}(x^{(i)})) x_j^{(i)}$$</p>
<p>上式看起来和<strong>线性回归的</strong>求解方法随机梯度下降（和梯度下降的不同是每次更新是随机选<strong>一个</strong>样本而非全部样本）的更新规则<strong>看起来</strong>完全一样：<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/stochastic-gradient-descent.png" alt=""></div></div><div class="group-picture-row"></div></div></div><br>但它们不是同一个算法，因为在随机梯度上升规则中的$h_{\theta}(x^{(i)})$是$\theta^Tx^{(i)}$的<strong>非线性</strong>函数，而线性回归中的是线性的。<br>尽管如此，两者形式上的相同并不是巧合，它们有更深层次的联系（Generalized Linear Models），感兴趣的读者可阅读<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">CS229 Lecture notes1 Part III</a>.</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>上文所讨论的梯度上升法针对的是最大化$l(\theta)$，是正式完整的逻辑回归求解过程。下面讨论梯度下降法，针对<strong>最小化代价函数</strong>。两者实质是一样的，或者说，梯度下降法优化的代价函数的表示可以（可能就是）从$l(\theta)$得来的。<br>算法如下：<br><img src="/images/reg-classification-kaggle/logistic-regression-gradient-descent.png" title="图十：逻辑回归的梯度下降法, modify from: Andrew Ng" style="margin:auto;"><br>$J(\theta)$表示代价函数，它在上文推导出的$l(\theta)$基础上乘一个负数$-\frac{1}{m}$将原来最大化的问题转化为最小化，这种转化是因为我们总是最小化代价函数（代价小）。在此基础上可以求导、迭代等，和线性回归中梯度下降的过程类似。</p>
<h3 id="梯度法分类及实现"><a href="#梯度法分类及实现" class="headerlink" title="梯度法分类及实现"></a>梯度法分类及实现</h3><p>下面以逻辑回归的梯度上升法为例，用Python语言分别实现批量梯度上升、随机梯度上升、小批量梯度上升法及批量梯度上升法的多进程并行化。<br>前文已经讨论过，逻辑回归的梯度上升法和梯度下降法实质是一样的，具体来说，下面代码（梯度上升）中的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">error = labelMat[i] - sigmoid(predict(weights, data))</div><div class="line"><span class="keyword">for</span> d <span class="keyword">in</span> data:</div><div class="line">    _weights[d] = _weights[d] + alpha*error*data[d]</div></pre></td></tr></table></figure>
<p>两行各换个符号便是梯度下降法了，实际相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">error = sigmoid(predict(t_weights, data)) - labelMat[i]</div><div class="line"><span class="keyword">for</span> d <span class="keyword">in</span> data:</div><div class="line">    _weights[d] = _weights[d] - alpha*error*data[d]</div></pre></td></tr></table></figure>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><ul>
<li>使用梯度法需要将数据进行特征缩放(feature scaling)！经测试，该步骤非常必要。后面的实现测试中，若未进行特征缩放，准确率总是为45%（比瞎猜还差），进行特征缩放后，第一次迭代的准确率即可超过50%.<br><img src="/images/reg-classification-kaggle/feature-scaling.png" title="图11：特征缩放, credit: Andrew Ng"><br>特征缩放作用的简化直观理解可参见上图。<br>本例中我使用了$\frac{x-min}{max-min}$对每个特征的值(x)进行处理，min和max分别是最小值和最大值。<br>值得注意的是，训练集和测试集都需要进行同样尺度的特征缩放。具体做法是先找到训练集的min,max，对训练集和测试集都用这个min,max缩放。</li>
<li>该例中的数据有一些异常：训练集的特征的index范围有1-201，测试集只有1-132.因此可预处理将训练集特征的index大于133的去除</li>
<li>训练集/测试集的样本从某一行开始重复（具体忘了，若要用此数据，可除重）</li>
</ul>
<h4 id="批量梯度上升法-Batch-Gradient-Ascent"><a href="#批量梯度上升法-Batch-Gradient-Ascent" class="headerlink" title="批量梯度上升法(Batch Gradient Ascent)"></a>批量梯度上升法(Batch Gradient Ascent)</h4><p>前面我们讨论的公式几乎都是批量梯度上升法，每采取一步更新都需要遍历<strong>全部训练集样本</strong>。当样本数较大时，成本较高。<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/LMS-rule.png" alt=""></div></div><div class="group-picture-row"></div></div></div><br>实现时要注意：</p>
<ul>
<li>权重（参数$\theta$）随机初始化的效果优于固定值初始化</li>
<li>在每次迭代要同步更新权重，因此需要先拷贝原权重用于该次迭代所有需更新的权重的计算，以免先计算的某个权重被用于其他权重的计算导致其他权重的计算是“后更新”的。（绕，看代码）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchGradAscent</span><span class="params">(dataMat, labelMat, valDataMat, valLabelMat, numIter = N_ITER)</span>:</span></div><div class="line">    alpha = ALPHA</div><div class="line">    weights = [random.random() <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)] <span class="comment"># random init</span></div><div class="line">    totalTime = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter):</div><div class="line">        start = time.time()</div><div class="line">        _weights = [weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)] <span class="comment"># copy</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataMat)):</div><div class="line">            dataDict = parseData(dataMat[i])</div><div class="line">            error = labelMat[i] - sigmoid(predict(weights, dataDict))</div><div class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> dataDict:</div><div class="line">                _weights[index] = _weights[index] + alpha*error*dataDict[index]</div><div class="line">        weights = [_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)] <span class="comment"># update</span></div><div class="line">        end = time.time()</div><div class="line">        totalTime = totalTime + float(end - start)</div><div class="line">        calcAcc(weights, valDataMat, valLabelMat, iter)</div><div class="line">    <span class="keyword">print</span> <span class="string">'batchGradAscent iter cost:'</span>, float(totalTime) / numIter, <span class="string">'s in average.'</span></div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure>
<h4 id="随机梯度上升法-Stochastic-Gradient-Ascent"><a href="#随机梯度上升法-Stochastic-Gradient-Ascent" class="headerlink" title="随机梯度上升法(Stochastic Gradient Ascent)"></a>随机梯度上升法(Stochastic Gradient Ascent)</h4><p>随机梯度上升法重复遍历训练集，每次遇到一个训练样本，都只根据和这<strong>一个训练样本</strong>相关的误差梯度(the gradient of the error)来更新参数。在训练集样本多的情况下，这种方法大大减小了计算成本，加快了收敛速度（但有可能永远收敛不到最小值而是一直在最小值周围震荡）。<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/images/reg-classification-kaggle/stochastic-gradient-descent.png" alt=""></div></div><div class="group-picture-row"></div></div></div><br>实现时要注意：</p>
<ul>
<li>该实现随机选取一个样本</li>
<li>该实现每次迭代随机采样的次数等于训练样本数（即公式中的m），这样在每次迭代中可随到大多数样本，样本丰富性高。如需加速，可减小采样数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent</span><span class="params">(dataMat, labelMat, valDataMat, valLabelMat, numIter = N_ITER)</span>:</span></div><div class="line">    alpha = ALPHA</div><div class="line">    weights = [random.random() <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">    totalTime = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter):</div><div class="line">        start = time.time()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataMat)):</div><div class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataMat)))</div><div class="line">            dataDict = parseData(dataMat[randIndex])</div><div class="line">            error = labelMat[randIndex] - sigmoid(predict(weights, dataDict))</div><div class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> dataDict:</div><div class="line">                weights[index] = weights[index] + alpha*error*dataDict[index]</div><div class="line">        end = time.time()</div><div class="line">        totalTime = totalTime + float(end - start)</div><div class="line">        calcAcc(weights, valDataMat, valLabelMat, iter)</div><div class="line">    <span class="keyword">print</span> <span class="string">'stocGradAscent iter cost:'</span>, float(totalTime) / numIter, <span class="string">'s in average.'</span></div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure>
<h4 id="小批量梯度上升法-Mini-batch-Gradient-Descent"><a href="#小批量梯度上升法-Mini-batch-Gradient-Descent" class="headerlink" title="小批量梯度上升法(Mini-batch Gradient Descent)"></a>小批量梯度上升法(Mini-batch Gradient Descent)</h4><p>这是批量梯度上升法和随机梯度上升法的折中方案。每次迭代随机选取n个样本更新参数。<br>实现时要注意：</p>
<ul>
<li>小批量的选取样本数大于1，每次迭代仍需同时更新参数，需要拷贝参数。</li>
<li>为了有较高的样本覆盖率，下面实现中设定每次迭代随机采样的次数为：$\frac{总训练样本数}{单批量数}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">minibatchGradAscent</span><span class="params">(dataMat, labelMat, valDataMat, valLabelMat,\</span></span></div><div class="line"> batch = <span class="number">10</span>, numIter = N_ITER):</div><div class="line">    alpha = ALPHA</div><div class="line">    weights = [random.random() <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">    totalTime = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter):</div><div class="line">        start = time.time()</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(dataMat) / batch):</div><div class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataMat)-batch))</div><div class="line">            _weights = [weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch):</div><div class="line">                dataDict = parseData(dataMat[randIndex+i])</div><div class="line">                error = labelMat[randIndex+i] - sigmoid(predict(weights, dataDict))</div><div class="line">                <span class="keyword">for</span> index <span class="keyword">in</span> dataDict:</div><div class="line">                    _weights[index] = _weights[index] + alpha*error*dataDict[index]</div><div class="line">            weights = [_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">        end = time.time()</div><div class="line">        totalTime = totalTime + float(end - start)</div><div class="line">        calcAcc(weights, valDataMat, valLabelMat, iter)</div><div class="line">    <span class="keyword">print</span> <span class="string">'minibatchGradAscent iter cost:'</span>, float(totalTime) / numIter, <span class="string">'s in average.'</span></div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure>
<h3 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h3><p>使用<strong>Python的多进程</strong>对逻辑回归的批量梯度上升法中计算梯度的过程进行并行化，达到加速运算的目的。</p>
<h4 id="选择方法"><a href="#选择方法" class="headerlink" title="选择方法"></a>选择方法</h4><ol>
<li>选择<strong>批量</strong>梯度上升法是因为每次迭代整个训练集都要计算梯度，但又互不影响，没有依赖关系和数据访问冲突问题。最终参数的更新在全部样本完成梯度计算之后进行汇合。</li>
<li><p>首先，要清楚所使用机器和并行化相关的配置，如在Linux系统中可通过<a href="http://manpages.courier-mta.org/htmlman1/lscpu.1.html" target="_blank" rel="external"><code>lscpu</code></a>命令查看和CPU架构相关的信息，了解核数、CPU数等。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]$ lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                72               <span class="comment"># 一个核可以包含多个CPU</span></div><div class="line">On-line CPU(s) list:   0-71</div><div class="line">Thread(s) per core:    2                <span class="comment"># 每个CPU核支持两个线程（超线程）</span></div><div class="line">Core(s) per socket:    18               <span class="comment"># 一个socket可以包含多个核</span></div><div class="line">Socket(s):             2                <span class="comment"># 共有两个socket</span></div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 79</div><div class="line">Model name:            Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz</div><div class="line"><span class="comment"># 省略了部分输出</span></div></pre></td></tr></table></figure>
<p> 或者使用更具体的命令<a href="http://www.cnblogs.com/emanlee/p/3587571.html" target="_blank" rel="external">查看Linux物理CPU个数、核数、逻辑CPU个数</a>：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 总核数 = 物理CPU个数 X 每个物理CPU的核数</span></div><div class="line"><span class="comment"># 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数</span></div><div class="line">$ cat /proc/cpuinfo| grep <span class="string">"physical id"</span>| sort| uniq| wc <span class="_">-l</span></div><div class="line">2                    <span class="comment"># 物理CPU个数为2</span></div><div class="line">$ cat /proc/cpuinfo| grep <span class="string">"cpu cores"</span>| uniq</div><div class="line">cpu cores   : 18     <span class="comment"># 每个物理CPU中core的个数(即核数)为18</span></div><div class="line">$ cat /proc/cpuinfo| grep <span class="string">"processor"</span>| wc <span class="_">-l</span></div><div class="line">72                   <span class="comment"># 逻辑CPU的个数为72</span></div></pre></td></tr></table></figure>
<p> 该服务器核数较多。</p>
</li>
<li><p>选择<strong>多进程</strong>而非多线程和Python并行化机制有关。有一个较为普遍的认识：</p>
<blockquote>
<p>python中的多线程其实并不是真正的多线程，如果想要充分地使用<strong>多核CPU</strong>的资源，在python中大部分情况需要使用<strong>多进程</strong>。（<a href="http://python.jobbole.com/82045/" target="_blank" rel="external">Python多进程编程</a>）</p>
</blockquote>
<p> 由于本例中逻辑回归使用Python编程，且所使用的服务器十分符合<strong>多核CPU</strong>，Python还提供了好用的多进程包<code>multiprocessing</code>，本例采用Python多进程来对逻辑回归的批量梯度上升法进行并行化。<br> 进一步参考<a href="http://bbs.51cto.com/thread-1349105-1.html" target="_blank" rel="external">为什么在Python里推荐使用多进程而不是多线程？</a>，在Python多线程下，每个线程的执行方式是<br> 1). 获取GIL(Global Interpreter Lock全局解释器锁)<br> 2). 执行代码直到sleep或者是python虚拟机将其挂起。<br> 3). 释放GIL</p>
<p> 也就是说，进入CPU执行代码需要<strong>获得GIL</strong>，而在一个Python进程中，GIL只有一个，即使一个进程中有多线程也只有获得GIL的线程才能执行。非但如此，在<strong>释放GIL</strong>时的线程锁竞争、切换线程的会<strong>消耗资源</strong>。<br> 而多进程的好处在于，每个进程有各自独立的GIL，互不干扰，利于并行执行，所以<em>在python中，多进程的执行效率优于多线程(仅仅针对多核CPU而言)</em>。 </p>
</li>
</ol>
<h4 id="实现多进程"><a href="#实现多进程" class="headerlink" title="实现多进程"></a>实现多进程</h4><p><code>multiprocessing</code>是Python的多进程包，支持子进程、通信和数据共享，提供了<code>Process</code>、<code>Queue</code>、<code>Pipe</code>、<code>Lock</code>等组件。下面将用到的是<code>Process</code>，类和方法如下：</p>
<blockquote>
<p><strong>创建进程的类</strong>：<code>Process([group [, target [, name [, args [, kwargs]]]]])</code><br><code>target</code>表示调用对象，<code>args</code>表示调用对象的位置参数元组。<code>kwargs</code>表示调用对象的字典。<code>name</code>为别名。<code>group</code>实质上不使用。<br><strong>方法</strong>：<code>is_alive()</code>、<code>join([timeout])</code>、<code>run()</code>、<code>start()</code>、<code>terminate()</code><br>其中，<code>Process</code>以<code>start()</code>启动某个进程。</p>
</blockquote>
<p>下面是主程序。和原批量梯度上升法实现的主要区别在于代码中间部分用到的<code>Process</code>类。<code>target=func</code>中的<code>func</code>是自定义函数，用于指定每个进程的任务，在该例中为<code>calcGrad</code>，即计算一部分样本梯度。<code>process</code>通过<code>start()</code>、<code>join()</code>等方法控制进程的开始、汇合。代码中还用到了<code>Lock()</code>，在多个进程需要访问共享资源<code>weights</code>时可以避免访问冲突。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchGradAscentMultiProcess</span><span class="params">(dataMat, labelMat, valDataMat, valLabelMat, \</span></span></div><div class="line">    processNum = <span class="number">18</span>, numIter = N_ITER):</div><div class="line">    _weights = Array(<span class="string">'f'</span>, range(N_FEATURE + <span class="number">1</span>))</div><div class="line">    alpha = ALPHA</div><div class="line">    weights = [random.random() <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)] <span class="comment"># random init</span></div><div class="line">    </div><div class="line">    sampleNum = len(dataMat)</div><div class="line">    <span class="keyword">if</span> sampleNum &lt; processNum:</div><div class="line">        processNum = sampleNum</div><div class="line">    step = sampleNum / processNum <span class="comment"># workload of each process</span></div><div class="line">    </div><div class="line">    totalTime = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter):</div><div class="line">        start = time.time()</div><div class="line">        lock = Lock()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights)): _weights[i] = weights[i]</div><div class="line"></div><div class="line">        processes = [] <span class="comment"># list</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, sampleNum, step):</div><div class="line">            <span class="keyword">if</span> i + step &gt; sampleNum:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            process = Process(target=calcGrad, \</div><div class="line">                args=(_weights, alpha, dataMat[i:i+step], labelMat[i:i+step], lock))</div><div class="line">            processes.append(process)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(processes)):</div><div class="line">            <span class="comment"># print 'Process ', i, ' started.'</span></div><div class="line">            processes[i].start()</div><div class="line"></div><div class="line">        <span class="comment"># join(): Block the calling thread until the process</span></div><div class="line">        <span class="comment"># whose join() method is called terminates</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(processes)):</div><div class="line">            processes[i].join()</div><div class="line">            <span class="comment"># print 'Process ', i, 'ended.'</span></div><div class="line"></div><div class="line">        weights = [_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">        end = time.time()</div><div class="line">        totalTime = totalTime + float(end - start)</div><div class="line">        calcAcc(weights, valDataMat, valLabelMat, iter)</div><div class="line">    <span class="keyword">print</span> <span class="string">'batchGradAscentMultiProcess iter cost:'</span>, float(totalTime) / numIter,\</div><div class="line">    <span class="string">'s in average. processNum:'</span>, processNum</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure>
<p><code>calcGrad</code>计算梯度在非并行化版本上加了两个步骤：</p>
<ul>
<li><code>_weights</code>是共享变量，<code>t_weights</code>是<code>_weights</code>的拷贝，先对<code>t_weights</code>进行各种操作，最后通过加锁来更新共享变量。由于多进程共享资源会引起进程间相互竞争，通过这种“拷贝”将复杂的操作转移到非共享变量上可以减少竞争对结果的不确定性影响。</li>
<li>更新共享变量前需加锁：<code>with lock</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcGrad</span><span class="params">(_weights, alpha, dataMat, labelMat, lock)</span>:</span></div><div class="line">    t_weights = [_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">    sums = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>)]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataMat)):</div><div class="line">        data = parseData(dataMat[i])</div><div class="line">        error = labelMat[i] - sigmoid(predict(t_weights, data))</div><div class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data:</div><div class="line">            sums[d] = sums[d] + alpha*error*data[d]</div><div class="line">    <span class="keyword">with</span> lock:</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N_FEATURE + <span class="number">1</span>):</div><div class="line">            _weights[i] += sums[i]</div></pre></td></tr></table></figure>
<h4 id="并行效果"><a href="#并行效果" class="headerlink" title="并行效果"></a>并行效果</h4><p><img src="/images/reg-classification-kaggle/multiprocess-parallel-time.gif" title="图12：批量梯度上升法多进程并行化时间" width="70%" style="margin:auto;"></p>
<p><strong>现象</strong>：</p>
<ol>
<li>进程数从1以两倍的关系递增到16的过程中用时基本呈<strong>减半</strong>关系下降</li>
<li>16进程到32进程的用时减少，但幅度不大</li>
<li>32进程左右再增大进程数不仅没有减少用时，反而略微增加了。</li>
</ol>
<p><strong>解释</strong>：</p>
<ol>
<li>符合预期，成功实现多进程并行化</li>
<li>进程数较多时，每个进程计算梯度的用时很小，那些用于构建并行化操作的基础用时（如start、join、lock）在总用时中的比重变大，变得可见。</li>
<li>所用服务器的总核数（不是逻辑总核数，多进程不计超线程的影响）是36，与32接近，能开启的进程数已饱和（服务器上的部分CPU也在进行别的任务）。反而增加了用时（猜测）是因为做了无用功。</li>
</ol>
<p><em>并行化和其他非并行化方法的对比，请见下文</em></p>
<h3 id="结果及比较"><a href="#结果及比较" class="headerlink" title="结果及比较"></a>结果及比较</h3><h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><p>使用逻辑回归之梯度上升法训练约50次（$\alpha=0.01）便难以再提升，预测出的label在0.5上下，因此直接四舍五入成0和1，提交结果<code>score =  0.72512</code>.后面将介绍的XGBoost的效果大幅(<code>score &gt;  0.9</code>)超越逻辑回归，因此不再继续对逻辑回归调参，仅作为一个方法比较。</p>
<h4 id="准确率比较"><a href="#准确率比较" class="headerlink" title="准确率比较"></a>准确率比较</h4><p><img src="/images/reg-classification-kaggle/logistic-regression-gradient-ascent-acc.gif" title="图13：逻辑回归之梯度上升法准确率" width="70%" style="margin:auto;"><br>上图是设$\alpha=0.1$设各方法迭代两次统计的准确率，准确率的计算方法是预测值大于等于0且标签为1或预测值小于0且标签为0时预测正确。该学习率设置得较大是为了使在两次迭代（节省运行时间）情况下就可以看出准确率的变化。<br>测试结果在一定程度上表现了</p>
<ul>
<li>并行化和非并行化版本的结果一致，说明实现过程是正确的。</li>
<li>随机梯度上升法和小批量梯度上升法均在第一次迭代就获得了较批量梯度上升法高许多的准确率，说明前者较后者“前进步伐”更大。</li>
<li>随机梯度上升法和小批量梯度上升法第二次迭代的准确率较第一次几乎没有变化，这（可能）是因为学习率过大，导致在极小值附近“震荡”。</li>
<li>批量梯度上升法在第二次迭代取得了长足的进步。</li>
</ul>
<h4 id="用时比较"><a href="#用时比较" class="headerlink" title="用时比较"></a>用时比较</h4><p><img src="/images/reg-classification-kaggle/logistic-regression-gradient-ascent-time.gif" title="图14：逻辑回归之梯度上升法耗时" width="70%" style="margin:auto;"><br>上图是四种方法单次迭代（由两次迭代用时平均得来）所用时间的比较。注意计时时不考虑测试准确率的用时（因为未对这个操作进行并行化，计入会影响并行化时间比较），但考虑了除此之外单次迭代中所用到的各种操作，包括复制权重、并行化的进程相关操作等。<br>测试结果在一定程度上表现了</p>
<ul>
<li>16进程并行化的批量梯度上升法无疑是最快速的，但未达到16倍的加速（约11倍），这与多进程需要额外的操作有关（如等待各子进程完成梯度计算才能更新权重）。</li>
<li>根据前面非并行化三种方法的实现可知，这三种方法都用了同样多次数的循环操作（迭代次数*总训练样本数），最终三者时间上仍有一定差异。</li>
</ul>
<h2 id="GBDT之XGBoost"><a href="#GBDT之XGBoost" class="headerlink" title="GBDT之XGBoost"></a>GBDT之XGBoost</h2><p>除了逻辑回归法，还有很多方法（例如梯度提升决策树GBDT）可用于分类，且效果往往更佳。和scikit-learn类似，XGBoost (eXtreme Gradient Boosting)也是造轮子的，使用较为简便。通过一个官方的二分类例子<a href="https://github.com/dmlc/xgboost/tree/master/demo/binary_classification" target="_blank" rel="external">XGBoost demo: binary classification</a>，即可上手（安装配置XGBoost可能相对耗时）。<br>最简单的使用方法如demo所示，不需要写代码，只需更改<code>.conf</code>文件中的参数设置，参考<code>.sh</code>文件中的命令运行即可。在默认的参数设置下都可取得不错（<code>score&gt;0.9</code>，远优于逻辑回归）的结果。但是要想大幅改进，就不得不开始调参再调参（以及可结合scikit-learn利用交叉验证等等）。</p>
<h3 id="输入数据格式"><a href="#输入数据格式" class="headerlink" title="输入数据格式"></a>输入数据格式</h3><p>XGBoost使用的是<a href="https://stats.stackexchange.com/questions/61328/libsvm-data-format" target="_blank" rel="external">LibSVM格式</a>，样例如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1 101:1.2 102:0.03</div><div class="line">0 1:2.1 10001:300 10002:400</div><div class="line">...</div></pre></td></tr></table></figure>
<p>每行代表一个样本，第一列是标签（类别），诸如<code>101:1.2</code>格式的表示第<code>101</code>号特征值为<code>1.2</code>.以此类推。<br>分类比赛的Kaggle数据集格式和XGBoost的要求不谋而合，因此无需改动数据格式。</p>
<h3 id="一般操作"><a href="#一般操作" class="headerlink" title="一般操作"></a>一般操作</h3><p>安装完成后，在<code>xgboost</code>文件夹下有一个<code>xgboost</code>文件，假设其绝对路径为<code>/home/xgboost/xgboost</code>.</p>
<p><code>.conf</code>文件是用来指定各种参数的，可以放在任意路径下，假设当前路径在<code>classification.conf</code>目录下。<code>classification.conf</code>文件如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># General Parameters, see comment for each definition</span></div><div class="line"><span class="comment"># can be gbtree or gblinear</span></div><div class="line">booster = gbtree</div><div class="line"><span class="comment"># choose logistic regression loss function for binary classification</span></div><div class="line">objective = binary:logistic</div><div class="line"></div><div class="line"><span class="comment"># Tree Booster Parameters</span></div><div class="line"><span class="comment"># step size shrinkage</span></div><div class="line">eta = <span class="number">0.01</span></div><div class="line"><span class="comment"># minimum loss reduction required to make a further partition</span></div><div class="line">gamma = <span class="number">0</span></div><div class="line"><span class="comment"># minimum sum of instance weight(hessian) needed in a child</span></div><div class="line">min_child_weight = <span class="number">7</span></div><div class="line"><span class="comment"># maximum depth of a tree</span></div><div class="line">max_depth = <span class="number">12</span></div><div class="line"></div><div class="line"><span class="comment"># Task Parameters</span></div><div class="line"><span class="comment"># the number of round to do boosting</span></div><div class="line">num_round = <span class="number">1000</span></div><div class="line"><span class="comment"># 0 means do not save any model except the final round model</span></div><div class="line">save_period = <span class="number">100</span></div><div class="line"><span class="comment"># set evaluation metric</span></div><div class="line">eval_metric = auc</div><div class="line"><span class="comment"># The path of training data</span></div><div class="line">data = <span class="string">"train10_9.txt"</span></div><div class="line"><span class="comment"># data = "train_data.txt"</span></div><div class="line"><span class="comment"># The path of validation data, used to monitor training process, </span></div><div class="line"><span class="comment"># here [val] sets name of the validation set</span></div><div class="line">eval[val] = <span class="string">"val10_1.txt"</span></div><div class="line"><span class="comment"># eval[train_full] = "train_data.txt"</span></div><div class="line"><span class="comment"># The path of test data</span></div><div class="line">test:data = <span class="string">"test_data.txt"</span></div><div class="line"></div><div class="line">model_dir=<span class="string">"models_etad01_ga0_mcw7_dep12"</span></div></pre></td></tr></table></figure>
<p>当前设置下的<code>data = &quot;train10_9.txt&quot;</code>和<code>eval[val] = &quot;val10_1.txt&quot;</code>是将完整的带标签的数据集以9:1比例分成了训练集和验证集用于调参，真正测试时需要改为<code>data = &quot;train_data.txt&quot;</code>来训练。在我的设置中，<code>model_dir=&quot;models_full_etad01_ga0_mcw7_dep12&quot;</code>用于指定生成模型的路径，文件名和设定的提升树参数(Tree Booster Parameters)一致，运行前需要先创建该目录。</p>
<p>常用的运行命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment"># training and output the models</span></div><div class="line">nohup /home/xgboost/xgboost classification.conf &gt;&gt; models_etad005_ga0_mcw9_dep12/nohup.out 2&gt;&amp;1 &amp;</div><div class="line"></div><div class="line"><span class="comment"># Continue from Existing Model</span></div><div class="line">nohup /home/xgboost/xgboost classification.conf model_in=/home/classification/models_etad01_ga0_mcw7_dep12/0900.model num_round=100 model_out=1000.model &gt;&gt; models_etad01_ga0_mcw7_dep12/nohup.out 2&gt;&amp;1 &amp;</div><div class="line"></div><div class="line"><span class="comment"># output prediction task=pred </span></div><div class="line">/home/xgboost/xgboost classification.conf task=pred model_in=/home/classification/models_etad01_ga0_mcw7_dep12/0800.model</div></pre></td></tr></table></figure>
<p>注意上面命令不需要同时使用，使用时把不需要的注释掉。</p>
<ol>
<li>训练：用<code>nohup</code>后台训练并将输出内容存到模型文件夹下的<code>nohup.out</code>中</li>
<li>继续训练：在已有模型<code>0900.model</code>基础上再训练100步</li>
<li>预测：用模型<code>0800.model</code>进行预测</li>
</ol>
<h3 id="调参-Parameter-Tuning"><a href="#调参-Parameter-Tuning" class="headerlink" title="调参(Parameter Tuning)"></a>调参(Parameter Tuning)</h3><p>参数主要分为三类：一般参数(General Parameters)、提树参数(Tree Booster Parameters)、任务参数(Task Parameters)，我们主要调节的是Tree Booster Parameters.调参方法具体可参考<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="external">Complete Guide to Parameter Tuning in XGBoost (with codes in Python)</a>.<br>一般来说分为以下几步：</p>
<ol>
<li>选择一个相对高的<code>eta</code>（一般可选0.1），在此基础上找到最优的<code>max_depth</code>.</li>
<li>在上一步基础上找到最优的<code>min_child_weight</code></li>
<li>在上一步基础上找到最优的<code>gamma</code></li>
<li>减小<code>eta</code>，加大<code>max_depth</code>训练。</li>
</ol>
<h1 id="Reference-amp-Futher-Reading"><a href="#Reference-amp-Futher-Reading" class="headerlink" title="Reference &amp; Futher Reading"></a>Reference &amp; Futher Reading</h1><ol>
<li><a href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B01ARKEV1G" target="_blank" rel="external">《机器学习》周志华著</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="external">斯坦福机器学习课程</a></li>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">CS229 Lecture notes1 by Andrew Ng</a></li>
<li><a href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html" target="_blank" rel="external">scikit-learn: Neural network models (supervised) - Multi-layer Perceptron</a></li>
<li><a href="https://scikit-learn.org/stable/modules/neighbors.html" target="_blank" rel="external">scikit-learn: Nearest Neighbors</a></li>
<li><a href="http://blog.163.com/jiangfeng_data/blog/static/20641403820125141026440/" target="_blank" rel="external">最近邻算法</a></li>
<li><a href="http://bbs.51cto.com/thread-1349105-1.html" target="_blank" rel="external">为什么在Python里推荐使用多进程而不是多线程？</a></li>
<li><a href="http://python.jobbole.com/82045/" target="_blank" rel="external">Python多进程编程</a></li>
<li><a href="https://docs.python.org/2/library/multiprocessing.html" target="_blank" rel="external">multiprocessing — Process-based “threading” interface</a></li>
<li><a href="https://github.com/dmlc/xgboost/tree/master/demo/binary_classification" target="_blank" rel="external">XGBoost demo: binary classification/</a></li>
<li><a href="http://cs.brynmawr.edu/Courses/cs380/spring2011/lectures/03-optimization.pdf" target="_blank" rel="external">optimization method</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="external">Complete Guide to Parameter Tuning in XGBoost (with codes in Python)</a></li>
<li><a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="external">KAGGLE ENSEMBLING GUIDE</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26820998" target="_blank" rel="external">【干货】Kaggle 数据挖掘比赛经验分享</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/11/SSAD-project-practice/" rel="next" title="系统分析与设计项目实践">
                <i class="fa fa-chevron-left"></i> 系统分析与设计项目实践
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="HYPJUDY" />
          <p class="site-author-name" itemprop="name">HYPJUDY</p>
           
              <p class="site-description motion-element" itemprop="description">What I cannot create, I do not understand.</p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/hypjudy" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录-Table-of-Contents"><span class="nav-number">1.</span> <span class="nav-text">目录(Table of Contents)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#回归-Regression"><span class="nav-number">2.</span> <span class="nav-text">回归(Regression)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据-Data"><span class="nav-number">2.1.</span> <span class="nav-text">数据(Data)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#度量标准-Evaluation"><span class="nav-number">2.2.</span> <span class="nav-text">度量标准(Evaluation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归-linear-regression"><span class="nav-number">2.3.</span> <span class="nav-text">线性回归(linear regression)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性模型-linear-model"><span class="nav-number">2.3.1.</span> <span class="nav-text">线性模型(linear model)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归-linear-regression-1"><span class="nav-number">2.3.2.</span> <span class="nav-text">线性回归(linear regression)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降法-Gradient-Descent"><span class="nav-number">2.3.3.</span> <span class="nav-text">梯度下降法(Gradient Descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法概观"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">算法概观</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度“下降”的含义"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">梯度“下降”的含义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度“下降”证明"><span class="nav-number">2.3.3.3.</span> <span class="nav-text">梯度“下降”证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算偏导数"><span class="nav-number">2.3.3.4.</span> <span class="nav-text">计算偏导数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#用MATLAB实现"><span class="nav-number">2.3.3.5.</span> <span class="nav-text">用MATLAB实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#读写数据及测试"><span class="nav-number">2.3.3.5.1.</span> <span class="nav-text">读写数据及测试</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#梯度下降实现"><span class="nav-number">2.3.3.5.2.</span> <span class="nav-text">梯度下降实现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#交叉验证"><span class="nav-number">2.3.3.5.3.</span> <span class="nav-text">交叉验证</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果"><span class="nav-number">2.3.3.6.</span> <span class="nav-text">结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正规方程-Normal-Equation"><span class="nav-number">2.3.4.</span> <span class="nav-text">正规方程(Normal Equation)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法概观-1"><span class="nav-number">2.3.4.1.</span> <span class="nav-text">算法概观</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算参数"><span class="nav-number">2.3.4.2.</span> <span class="nav-text">计算参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MATLAB实现"><span class="nav-number">2.3.4.3.</span> <span class="nav-text">MATLAB实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果-1"><span class="nav-number">2.3.4.4.</span> <span class="nav-text">结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多层感知器-Multi-layer-Perceptron-MLP"><span class="nav-number">2.4.</span> <span class="nav-text">多层感知器(Multi-layer Perceptron, MLP)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型概观"><span class="nav-number">2.4.1.</span> <span class="nav-text">模型概观</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用TensorFlow实现"><span class="nav-number">2.4.2.</span> <span class="nav-text">用TensorFlow实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#主框架"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">主框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最近邻之scikit-learn"><span class="nav-number">2.5.</span> <span class="nav-text">最近邻之scikit-learn</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN概览"><span class="nav-number">2.5.1.</span> <span class="nav-text">KNN概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用scikit-learn实现"><span class="nav-number">2.5.2.</span> <span class="nav-text">用scikit-learn实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trick"><span class="nav-number">2.5.3.</span> <span class="nav-text">Trick</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#各回归方法对比"><span class="nav-number">2.6.</span> <span class="nav-text">各回归方法对比</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分类-Classification"><span class="nav-number">3.</span> <span class="nav-text">分类(Classification)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据-Data-1"><span class="nav-number">3.1.</span> <span class="nav-text">数据(Data)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#度量标准-Evaluation-1"><span class="nav-number">3.2.</span> <span class="nav-text">度量标准(Evaluation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归-Logistic-Regression"><span class="nav-number">3.3.</span> <span class="nav-text">逻辑回归(Logistic Regression)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hypothesis-Representation"><span class="nav-number">3.3.1.</span> <span class="nav-text">Hypothesis Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度上升法"><span class="nav-number">3.3.2.</span> <span class="nav-text">梯度上升法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Likelihood-of-the-Parameters"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">Likelihood of the Parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Maximize-the-Likelihood-Gradient-Ascent"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">Maximize the Likelihood (Gradient Ascent)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降法"><span class="nav-number">3.3.3.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度法分类及实现"><span class="nav-number">3.3.4.</span> <span class="nav-text">梯度法分类及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据预处理"><span class="nav-number">3.3.4.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#批量梯度上升法-Batch-Gradient-Ascent"><span class="nav-number">3.3.4.2.</span> <span class="nav-text">批量梯度上升法(Batch Gradient Ascent)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度上升法-Stochastic-Gradient-Ascent"><span class="nav-number">3.3.4.3.</span> <span class="nav-text">随机梯度上升法(Stochastic Gradient Ascent)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#小批量梯度上升法-Mini-batch-Gradient-Descent"><span class="nav-number">3.3.4.4.</span> <span class="nav-text">小批量梯度上升法(Mini-batch Gradient Descent)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#并行化"><span class="nav-number">3.3.5.</span> <span class="nav-text">并行化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#选择方法"><span class="nav-number">3.3.5.1.</span> <span class="nav-text">选择方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现多进程"><span class="nav-number">3.3.5.2.</span> <span class="nav-text">实现多进程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#并行效果"><span class="nav-number">3.3.5.3.</span> <span class="nav-text">并行效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果及比较"><span class="nav-number">3.3.6.</span> <span class="nav-text">结果及比较</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结果-2"><span class="nav-number">3.3.6.1.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#准确率比较"><span class="nav-number">3.3.6.2.</span> <span class="nav-text">准确率比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#用时比较"><span class="nav-number">3.3.6.3.</span> <span class="nav-text">用时比较</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT之XGBoost"><span class="nav-number">3.4.</span> <span class="nav-text">GBDT之XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#输入数据格式"><span class="nav-number">3.4.1.</span> <span class="nav-text">输入数据格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一般操作"><span class="nav-number">3.4.2.</span> <span class="nav-text">一般操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调参-Parameter-Tuning"><span class="nav-number">3.4.3.</span> <span class="nav-text">调参(Parameter Tuning)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference-amp-Futher-Reading"><span class="nav-number">4.</span> <span class="nav-text">Reference & Futher Reading</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HYPJUDY</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'hypjudy';
      var disqus_identifier = '2017/06/23/regression-classification-kaggle/';

      var disqus_title = "[数据挖掘] 回归和分类Kaggle实战";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      
        var disqus_config = function () {
            this.page.url = disqus_url;
            this.page.identifier = disqus_identifier;
            this.page.title = disqus_title;
        };
        run_disqus_script('embed.js');
      

    </script>
  










  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
